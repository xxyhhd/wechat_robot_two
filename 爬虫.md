爬虫分类：　通用爬虫、聚焦爬虫

１、定位ＵＲＬ
２、

## urllib

### 实例一
```python
import urllib.request
import urllib.parse  # 解决参数配置问题

url = 'http://www.baidu.com/s?'
data = {
    'wd': '美女',
    'pass': '性别'
}

urllib.request.urlretrieve(url=url, filename='baidu.html')　　# 保存到本地

data1 = urllib.parse.urlencode(data)
url = url + data1

print(url)
```

### 实例二
```python
import urllib.request

url = 'http://www.baidu.com'
response = urllib.request.urlopen(url=url)
print(type(response))  # 获取类型, <class 'http.client.HTTPResponse'>
print(response.getcode())  # 　获取状态码, 200
print(response.geturl()) # 获取ＵＲＬ
print(response.getheaders())  # 获取头信息

# 编码：object-->字节　encode：对字典类型，　quote：　对单个字符串
# 解码：字节-->object  decode：对字典类型， unquote：　对单个字符串
print(response.read().decode('utf-8'))  # utf-8解码,获取页面源码，readline()读取一行，readlines(num)读取多行
```

### 实例三，修改头信息
```python
import urllib.request

url = 'http://www.baidu.com'
headers = {
    'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.100 Safari/537.36'
}

request = urllib.request.Request(url=url, headers=headers)  # request 相当于url, 可以加其他参数
respones = urllib.request.urlopen(request)
print(respones.getcode())
```

### 爬取豆瓣电影
```python
import urllib.request
import urllib.parse
import json


def get_tag():
    url = 'https://movie.douban.com/j/search_tags?type=movie&source='
    respones = urllib.request.urlopen(url=url)
    content = respones.read().decode('utf-8')
    my_json = json.loads(content)
    info = '  '.join(my_json['tags'])
    print('可选电影种类：', info)
    return my_json['tags']


def create_request(a):
    tag = str(input('请输入查询的电影类型：'))
    cate = urllib.parse.quote(tag)
    num = int(input('请输入你需要查询的数量：'))
    page = num//20
    remainder = num % 20
    print('{0:^5}\t{1:{3}^３0}\t{2:^８}'.format('排名', '电影名称', '评分', chr(12288)))
    for x in range(page):
        data = {
            'page_start': x*20,
        }
        data = data = urllib.parse.urlencode(data)
        url = 'https://movie.douban.com/j/search_subjects?type=movie&page_limit=20&sort=rank&' + \
            'tag=' + cate + '&' + data
        respones = urllib.request.urlopen(url=url)
        content = respones.read().decode('utf-8')
        my_json = json.loads(content)
        count = x*20
        for move in my_json['subjects']:
            count += 1
            print('{0:^5}\t{1:{3}^３0}\t{2:^10}'.format(
                count, move['title'], move['rate'], chr(12288)))
    url = 'https://movie.douban.com/j/search_subjects?type=movie&page_limit=20&sort=rank&' + \
        'tag=' + cate + '&' + data
    respones = urllib.request.urlopen(url=url)
    content = respones.read().decode('utf-8')
    my_json = json.loads(content)
    count = page*20
    for move in my_json['subjects'][0:remainder]:
        count += 1
        print('{0:^5}\t{1:{3}^３0}\t{2:^10}'.format(
            count, move['title'], move['rate'], chr(12288)))


def main():
    # get_tag()
    create_request(get_tag())


if __name__ == '__main__':
    main()
```

### habdler
HTTPHandler, 普通handler对象，可以配置一般请求  
HTTPCookieProcessor,　配置cookie  
HTTPProxyHandler, 配置代理  

```python
import urllib.request
import urllib.parse

url = 'http://www.baidu.com'

headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.100 Safari/537.36',
}

req = urllib.request.Request(url=url, headers=headers)

# 构造更高级的请求对象
handler = urllib.request.HTTPHandler()
opener = urllib.request.build_opener(handler)
response = opener.open(req)

print(response.read().decode('utf-8'))
```

### 人人网，登录
```python
import urllib.request
import urllib.parse
import http.cookiejar
import json


cookie = http.cookiejar.CookieJar()
handler = urllib.request.HTTPCookieProcessor(cookie)
opener = urllib.request.build_opener(handler)


post_url = 'http://www.renren.com/ajaxLogin/login?1=1&uniqueTimestamp=2019651354277'
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.100 Safari/537.36',
}
form_data = {
    'email': '15620569943',
    'icode': '', 
    'origURL': 'http://www.renren.com/home',
    'domain': 'renren.com',
    'key_id': '1',
    'captcha_type': 'web_login',
    'password': '859a12c8d87dd25ee379ae206146390020d298612535bbb59cf29bacd5e2402e',
    'rkey': 'd0cf42c2d3d337f9e5d14083f2d52cb2',
    'f':'', 
}
# post做数据编码
data = urllib.parse.urlencode(form_data).encode('utf-8')
req = urllib.request.Request(data=data, url=post_url, headers=headers)
response = opener.open(req)

print(response.getcode())

my_url = 'http://www.renren.com/551403626/profile'
my_req = urllib.request.Request(url=my_url)
my_response = opener.open(my_req)
print(my_response.read().decode('utf-8'))
```

### Xpath语法

xpath 使用：   
+ 安装lxml: pip install lxml   
+ 导入：from lxml import etree   
+ 使用：html_tree = etree.pares('html文件')　or html_tree = etree.HTML(response.read().decode('utf-8'))   
+ xpant常用规则
    +   nodename	    选择这个节点名的所有子节点
    +       /	            从当前节点选择直接子节点
    +       //	        从当前节点选取子孙节点
    +       .	            选择当前节点
    +       …	            选取当前节点的父节点
    +       @	            选取属性
+ html = etree.parse('./test.html',etree.HTMLParser())  
    + result = html.xpath('//ul//a/text()') 获取所有ul节点下的所有a节点的文本部分
    + result = html.xpath('//li[@class="item-3"]') 获取所有class属性值为item-3的li节点,返回列表形式
    + result = html.xpath('//li[contains(@class,"item-0") and @name="one"]/a/text()') 多属性查询用contains



### Beautiful Soup 4
Beautiful Soup支持的解析器   
1. Python标准库	
    + 使用：BeautifulSoup(markup, “html.parser”) 
    + 优点：Python的内置标准库、执行速度适中、文档容错能力强 
    + 缺点：Python 2.7.3及Python 3.2.2之前的版本文档容错能力差
2. xml HTML解析器	
    + 使用：BeautifulSoup(markup, “lxml”) 
    + 优点：速度快、文档容错能力强 
    + 缺点：需要安装C语言库
3. lxml XML解析器	
    + 使用：BeautifulSoup(markup, “xml”) 
    + 优点：速度快、唯一支持XML的解析器 
    + 缺点：需要安装C语言库
4. html5lib	
    + 使用：BeautifulSoup(markup, “html5lib”) 
    + 优点：最好的容错性、以浏览器的方式解析文档、生成HTML5格式的文档 
    + 缺点：速度慢、不依赖外部扩展
#### 使用
+ 导入：from bs4 import BeautifulSoup  
+ 创建soup对象：soup = BeautifulSoup(open('html文件'), 'lxml') or soup = BeautifulSoup(抓取的页面, 'lxml')    
+ 优雅的打印HTML: print(soup.prettify())  
1. 节点选择器， 直接调用节点的名称就可以选择节点元素
    + 嵌套选择：每一个返回结果都是bs4.element.Tag类型，它同样可以继续调用节点进行下一步的选择。比如，我们获取了head节点元素，
    我们可以继续调用head来选取其内部的head节点元素
    + 获取某个元素：print(soup.p) 又多个p元素时之后选择第一个
    + 获取元素的类型：print(type(soup.title))  <class 'bs4.element.Tag'>
    + 获取元素的文本部分：print(soup.title.string) or soup.get_text() 推荐使用get
    + 获取元素的name: print(soup.title.name)
    + 获取元素的全部属性：print(soup.p.attrs)  --{'class': ['title'], 'name': 'dromouse'}
    + print(soup.p.attrs['name'])  --dromouse  
    <!-- 这里需要注意的是，有的返回结果是字符串，有的返回结果是字符串组成的列表。比如，name属性的值是唯一的，  
    返回的结果就是单个字符串。而对于class，一个节点元素可能有多个class，所以返回的是列表。在实际处理过程中，我们要注意判断类型 -->
2. 关联选择：在做选择的时候，有时候不能做到一步就选到想要的节点元素，需要先选中某一个节点元素，然后以它为基准再选择它的子节点、父节点、兄弟节点等
    + 获取p节点的所有直接子节点，返回生成器：soup.p.children
    + 获取p节点的所有直接子节点，返回列表：soup.p.contents
    + 获取p节点的所有所有子孙节点，返回生成器：soup.p.descendants
    + 获取a节点的直接父节点：soup.a.parent
    + 获取a节点的所有祖辈节点，返回生成器：soup.a.parents
    + next_sibling和previous_sibling分别获取节点的下一个和上一个兄弟元素
    + next_siblings和previous_siblings则分别返回所有前面和后面的兄弟节点的生成器
    <!-- 如果返回结果是单个节点，那么可以直接调用string、attrs等属性获得其文本和属性；如果返回结果是多个节点的生成器，则可以转为列表后取出某个元素，然后再调用string、attrs等属性获取其对应节点的文本和属性 -->
3. 方法选择器
    + find_all(name , attrs , recursive , text , **kwargs)
        + name节点类型：soup.find_all(name='ul')  ，查询所有ul节点，返回结果是列表类型
        + attrs属性查询: soup.find_all(attrs={'id': 'list-1'})，返回列表类型
        + 对于一些常用的属性，比如id和class等，我们可以不用attrs来传递
            + soup.find_all(id='list-1'))，返回列表类型
            + soup.find_all(class_='element')，返回列表类型
        + text匹配节点的文本，传入的形式可以是字符串，可以是正则表达式对象: soup.find_all(text=re.compile('link'))
    + find(), 返回匹配的第一个元素
        + soup.find(name='ul'
        + soup.find(class_='list')
    + find_parents()和find_parent()：前者返回所有祖先节点，后者返回直接父节点。 
    + find_next_siblings()和find_next_sibling()：前者返回后面所有的兄弟节点，后者返回后面第一个兄弟节点。 
    + find_previous_siblings()和find_previous_sibling()：前者返回前面所有的兄弟节点，后者返回前面第一个兄弟节点。 
    + find_all_next()和find_next()：前者返回节点后所有符合条件的节点，后者返回第一个符合条件的节点。 
    + find_all_previous()和find_previous()：前者返回节点后所有符合条件的节点，后者返回第一个符合条件的节点
4. CSS选择器，只需要调用select()方法，传入相应的CSS选择器即可
    + soup.select('ul li')
    + 部分CSS选择器
        + .class   
        .intro   
        选择class="intro"的所有元素。	
        + #id   
        #firstname   
        选择id="firstname"的所有元素。	       
        + element  
         p  
        选择所有p元素。	
        + element,element   
        div,p  
         选择所有div元素和所有p元素。
        + element element   
        div p   
        选择div元素内部的所有p元素。	
        + element>element  
         div>p 选择父元素为   
         div元素的所有p元素。
        + element+element  
         div+p   
         选择紧接在div元素之后的所有p元素。
        + attribute	        
        target	        
        选择带有target属性所有元素。
        + attribute=value	    
        target=_blank	    
        选择target="_blank"的所有元素。
        + attribute~=value	  
        title~=flower    
        选择title属性包含单词"flower"的所有元素。	
        + attribute|=value        
        lang|=en	        
        选择lang属性值以"en"开头的所有元素。

### bgk 和　gb123区别：都支持中文，但是gbk支持繁体中文

### selenium:动态加载JS,selenium可以模拟真实浏览器，自动化测试工具，支持多种浏览器，爬虫中主要用来解决JavaScript渲染问题.

1. 下载驱动器：http://chromedriver.storage.googleapis.com/index.html
2. 安装selenium包：pip install selenium　　　
#### 基本使用：
```python
from selenium import webdriver
import time

driver_path = '/home/rain/my_soft/chromedriver'  # 谷歌浏览器驱动路径
browser = webdriver.Chrome(driver_path) # 模拟谷歌浏览器

time.sleep(1)

browser.get('http://www.baidu.com')
browser.save_screenshot('baidu.png')  # 保存浏览器快照

browser.quit()　  # 千万记得退出
```
#### 进阶使用：

print(browser.page_source)  打印源码
1. 元素查找方法
    + find_element_by_name
    + find_element_by_id
    + find_element_by_xpath
    + find_element_by_link_text
    + find_element_by_partial_link_text
    + find_element_by_tag_name
    + find_element_by_class_name
    + find_element_by_css_selector
    + 或者使用下面的方法：
        + from selenium.webdriver.common.by import By
        first = browser.find_element(By.ID,"q"), By.ID可以换成上面的任意一个
    + find_elements_by_name 可以获取符合所有符合条件的元素，返回列表
2. 元素交互操作
browser.execute_script('document.documentElement.scrollTop=10000') ＃　模拟浏览器滚动条
```python

from selenium import webdriver

import time

browser = webdriver.Chrome()
browser.get("http://www.taobao.com")
input_str = browser.find_element_by_id('q')  # 定位到搜索输入框
input_str.send_keys("ipad")  # 输入ipad等待搜索
time.sleep(1)
input_str.clear()　# 后悔了，删除ipad
input_str.send_keys("MakBook pro") # 输入makbook pro
button = browser.find_element_by_class_name('btn-search')  # 定位到搜索按钮
button.click()　# 点击搜索按钮
```

### plantomJS, 无头浏览器，不会显示浏览器访问过程，现在已经不再使用，现在无头浏览器使用的是handless
#### handless
```python
from selenium import webdriver
import time
# from selenium .webdriver.chrome.options import Options

chrome_options = webdriver.ChromeOptions()
chrome_options.add_argument('--headless')
chrome_options.add_argument('--disable-gpu')
chrome_options.add_argument('--no-sandbox')

# chrome_path = '/usr/bin/google-chrome-stable'  # 本地chrome位置，因为已经添加到path,所以不需要
# chrome_options.binary_location = chrome_path
browser = webdriver.Chrome(chrome_options=chrome_options, executable_path='/home/rain/my_soft/chromedriver')

# url = 'www.baidu.com'
browser.get('https://www.baidu.com')
button = browser.find_element_by_id('su')
print(button.tag_name)
browser.quit()
```
### request模块
```python
import requests

url = 'https://www.baidu.com'
headers = {

}
data = {

}
response = requests.get(url=url, headers=headers，params=data)  # get 请求
response.encoding = 'utf-8'  # 设置编码格式

print(response.raw.read())  # 打印原始数据
print(response.text)  # 打印源码
print(response.url)  # 获取请求的url
print(response.status_code)  # 获取响应状态码
print(response.headers)　  # 获取响应头信息
```
#### requests 配置代理：
```python
proxies = {
  "http": "http://10.10.1.10:3128",
  "https": "http://10.10.1.10:1080",
}
requests.get(url, proxies=proxies)
```
proxies = {"http": "http://user:pass@10.10.1.10:3128/",}  ＃　用户名，密码
#### post请求
```python
def main(query):
    post_url = 'https://cn.bing.com/ttranslatev3?isVertical=1&&IG=B8917D88F08946B3B2E59191AF2156D9&IID=translator.5028.4'

    headers = {
        'user-agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.100 Safari/537.36',
        'cookie': '_EDGE_V=1; MUID=1C332E1A621D673A2E5823956333661A; SRCHD=AF=NOFORM; SRCHUID=V=2&GUID=10B713BC0506411887ABFECB2FB1C692&dmnchg=1; MUIDB=1C332E1A621D673A2E5823956333661A; _tarLang=default=en; _EDGE_S=SID=294018E5848C60360633157885A261BE; SNRHOP=I=&TS=; SRCHUSR=DOB=20190704&T=1563429442000; SRCHHPGUSR=CW=1853&CH=932&DPR=1&UTC=480&WTS=63699026242; MSCC=1; MSTC=ST=1; _SS=SID=294018E5848C60360633157885A261BE&HV=1563429482',
    }
    to = "en" if check_chinese(query) else 'zh-Hans'
    data = {
        'fromLang': 'auto-detect',
        'text': query,
        'to': to
    }
    response = requests.post(url=post_url, headers=headers, data=data)
    res = response.json()[0]["translations"][0]['text']
    # print(res)
    return res
```
### requests的cookie
```python
import requests

url_login = ''
headers = {
    'user-agent': '',
}
data = {

}
s = requests.session()  #　保存登录的session

s.post(url=url_login, headers=headers, data=data)  
print(s.cookies)
url_action = ''
s.post(url=url_action, headers=headers)  # 使用ｓ里面的session认证
```
### 验证码　　
常见的验证码形式：　　
+ 数字、字母图片
+ 有干扰的数字、字母图片
+ 图像
+ 滑动验证
+ 答题验证
+ 短信验证

```python
'''
登录时需要验证码，每次访问验证码链接都会更换，需要使用session去下载验证码图片
登录表单有隐藏项，且每次都随机刷新，需要先用session去获取当前的隐藏值
'''

import requests
from bs4 import BeautifulSoup


s = requests.session()  # 定制session

url = 'https://so.gushiwen.org/user/login.aspx?from=http://so.gushiwen.org/user/collect.aspx'
headers = {
    'user-agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.100 Safari/537.36',
}

r = s.get(url=url, headers=headers)

soup = BeautifulSoup(r.text, 'lxml')
value_1 = soup.select('#__VIE')[0].attrs.get('value') # form表单里的必须选项，服务器随机生成

img_url = 'http://so.gushiwen.org' + soup.select('#id2')[0].attrs.get('src')　# 获取验证码的链接

img = s.get(img_url)　

with open('./randcode.jpg', 'wb') as fp: # 二进制写
    fp.write(img.content) # 二进制写入

code = '通过第三方验证平台获取验证码'

data = {
    'value_1': value_1,
    'username': 'xxy',
    'password': 'www',
    'code': code
}

r = s.post(url=url, data=data, headers=headers)
with open('login.html', 'w', encoding='utf-8') as fp:
    fp.write(r.text)
```

### srcapy项目
1. 新建项目：scrapy startproject project_name
2. 新建蜘蛛：scrapy genspider spider_name url # 需要先进入项目文件夹下
3. 启动爬虫：scrapy crawl spilder_name
4. 在新建的spider文件里写爬虫代码
5. 目录结构：
    + items.py: 从网上爬取的数据存储在这里，数据规格化
    + middlewares.py: 中间件，连接数据库等功能
    + piplines.py: 管道，把数据转到某一个本地或者库里
    + settings.py: 配置文件
        + ROBOTSTXT_OBEY = False
        +  开启管道
            ITEM_PIPELINES = {
                'qiubai.pipelines.QiubaiPipeline': 300,
            }
        + 配置默认请求头：
            DEFAULT_REQUEST_HEADERS = {
            #   'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            #   'Accept-Language': 'en',
                'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.100 Safari/537.36',
            }
#### scrapy工作流程
1. 用scrapy框架的时候，一定要先明白执行的顺序，代码已写好，程序开始运行~
2. SPIDERS的yeild将request发送给ENGIN
3. ENGINE对request不做任何处理发送给SCHEDULER
4. SCHEDULER( url调度器)，生成request交给ENGIN
5. ENGINE拿到request，通过MIDDLEWARE进行层层过滤发送给DOWNLOADER
6. DOWNLOADER在网上获取到response数据之后，又经过MIDDLEWARE进行层层过滤发送给ENGIN
7. ENGINE获取到response数据之后，返回给SPIDERS，SPIDERS的parse()方法对获取到的response数据进行处理，解析出items或者requests
8. 将解析出来的items或者requests发送给ENGIN
9. ENGIN获取到items或者requests，将items发送给ITEMPIPELINES，将requests发送SCHEDULER注意！只有当调度器中不存在任何request了，整个程序才会停止，（也就是说，对于下载失败的URL，Scrapy也会重新下载。）
+ 引擎：Hi！Spider, 你要处理哪一个网站？
+ Spider：老大要我处理xxxx.com。
+ 引擎：你把第一个需要处理的URL给我吧。
+ Spider：给你，第一个URL是xxxxxxx.com。
+ 引擎：Hi！调度器，我这有request请求你帮我排序入队一下。
+ 调度器：好的，正在处理你等一下。
+ 引擎：Hi！调度器，把你处理好的request请求给我。
+ 调度器：给你，这是我处理好的request
+ 引擎：Hi！下载器，你按照老大的下载中间件的设置帮我下载一下这个request请求
+ 下载器：好的！给你，这是下载好的东西。（如果失败：sorry，这个request下载失败了。然后引擎告诉调度器，这个request下载失败了，你记录一下，我们待会儿再下载）
+ 引擎：Hi！Spider，这是下载好的东西，并且已经按照老大的下载中间件处理过了，你自己处理一下（注意！这儿responses默认是交给def parse()这个函数处理的）
+ Spider：（处理完毕数据之后对于需要跟进的URL），Hi！引擎，我这里有两个结果，这个是我需要跟进的URL，还有这个是我获取到的Item数据。
+ 引擎：Hi ！管道 我这儿有个item你帮我处理一下！调度器！这是需要跟进URL你帮我处理下。然后从第四步开始循环，直到获取完老大需要全部信息。
+ 管道``调度器：好的，现在就做！

#### parse方法
```python
class Test1Spider(scrapy.Spider):
    name = 'test1'
    allowed_domains = ['http://www.qiushibaike.com/hot/']
    start_urls = ['http://http://www.qiushibaike.com/hot//']

    def parse(self, response):
        items = []
        name = '' # 获取网页的name
        age = '' # 获取age

        item = QiubaiItem(name=name, age=age)
        items.append(item)
        return(items)
```
#### 创造实体的标志性写法
```python
items.py
class QiubaiItem(scrapy.Item):
    # define the fields for your item here like:
    # name = scrapy.Field()
    # 需要跟spider文件中的pares函数解析的数据格式匹配
    # 也就是当前类的属性名称应该跟parse函数中封装的字典对象的键一致
    name = scrapy.Field()
    age = scrapy.Field()
```
#### piplines.py(新建的管道需要在settings文件中添加)
```python
import json
import pymysql

# 保存到本地


class QiubaiPipeline(object):

    # 爬起文件开始执行时自动执行
    def open_spider(self, spider):
        self.fp = open('qiubai.json', 'w', encoding='utf-8')

    # 爬起文件结束时自动执行
    def close_spilder(self, spider):
        self.fp.close()

    # 对spider文件中的parse方法返回的可迭代对象进行迭代处理
    def process_item(self, item, spider):
        json_str = json.dumps(dict(item), ensure_ascii=False)
        self.fp.write(json_str + '\n')
        return item


# 连接数据库
class ConToMysqlPiplinr():
    def open_spider(self., spider):
        self.cont = pymysql.connect(
            host='127.0.0.1',
            port='3306',
            user='Rain',
            password='123456',
            db='scrapycontomysql',
            charset='utf-8'
        )
        self.cur = self.cont.cursor()

    def close_spilder(self, spider):
        self.cur.close()
        self.cont.close()

    def process_item(self, item, spider):
        sql = 'INSERT INTO table_name VALUES("%s","%s")'%(item[name],item[age])
        self.cur.execute(sql)
        self.cont.commit()
        return item
```

#### scrapy　shell; 用来单步调成
1. 启动shell: scrapy shell url
2. response.url: 响应的url
3. response.status: 响应状态码
4. response.headers:　请求头
5. response.text: 响应内容
6. response.body: 响应body
7. view(response): 在浏览器中查看页面
8. 根据xpath查找: author = response.xpath()
9. 根据css查找: author = response.csst()

#### 导出
scrapy crawl spilder_name -o filename.json
scrapy crawl spilder_name -o filename.xml
scrapy crawl spilder_name -o filename.csv　　　

中间件配置代理，cookie,selenium等动态信息的抓取与配置

### crawspider项目，是定制的爬虫
新建蜘蛛：scrapy genspider -t crawl crawspider url
相比普通的spider可以对爬起的链接设置规则
```python
class CrawspiderSpider(CrawlSpider):
    name = 'crawspider'
    allowed_domains = ['www.dushu.com']
    start_urls = ['http://www.dushu.com/']
    # allow是匹配返回多个符合该正则的链接
    # follow: 是否跟踪
    # callback 对每个url进行数据解析，回调：
    rules = (
        Rule(LinkExtractor(allow=r'/book/1077_\d+\.html'), callback='parse_item', follow=True),　# 符合规则回调到parse_item
    )

    def parse_item(self, response):
        items = {}
        #item['domain_id'] = response.xpath('//input[@id="sid"]/@value').get()
        #item['name'] = response.xpath('//div[@id="name"]').get()
        #item['description'] = response.xpath('//div[@id="description"]').get()
        book_list = response.xpath('所有书的路径')
        for bookitem in book_list:
            item = BookItem() # Items.py的类
            item['title'] = bookitem.xpath('title的路径').extract_first()
            next_url = 'www.dushu.com' + bookitem.xpath('详情页的路径').extract_first()
            yield scrapy.Request(url=next_url, callback=self.pares_next, meta={'book':item})
        return items

    def parse_next(self, response):
        item = response.meta('book')
        item['price'] = response.xpath('价格的路径')
        yield item
```

### scrapy-redis
scrapy-redis重写了管道，spider和调度器

